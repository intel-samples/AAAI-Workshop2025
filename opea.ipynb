{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OPEA ChatQnA Blueprint with Milvus as a Vector Database Deployment (DOCKER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following notebook will instruct you to deploy a RAG chatbot based on OPEA blueprint ChatQnA blueprints using Docker containers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro : The Open Platform for Enterprise (OPEA) Project \n",
    "OPEA uses microservices to create high-quality GenAI applications for enterprises, simplifying the scaling and deployment process for production. These microservices leverage a service composer that assembles them into a megaservice thereby creating real-world enterprise AI applications.\n",
    "\n",
    "It’s important to familiarize yourself with the MAIN key elements of OPEA:\n",
    "\n",
    "1. [**GenAIComps**](https://github.com/opea-project/GenAIComps) \n",
    "A collection of microservice components that form a service-based toolkit. Each microservice is designed to perform a specific function or task within the GenAI application architecture. By breaking down the system into these smaller, self-contained services, microservices promote modularity, flexibility, and scalability. This modular approach allows developers to independently develop, deploy, and scale individual components of the application, making it easier to maintain and evolve over time. All of the microservices are containerized, allowing cloud native deployment. Here, you will find contributions to multiple partners/communities to further construction.\n",
    "\n",
    "2. [**GenAIExamples**](https://github.com/opea-project/GenAIExamples)\n",
    "While *GenAIComps* offers a range of microservices, *GenAIExamples* provides practical, deployable solutions to help users implement these services effectively. This repo provides use-case-based applications that demonstrate how the OPEA architecture can be leveraged to build and deploy real-world GenAI applications. In the repo, developers can find practical resources such as Docker Compose files and Kubernetes Helm charts, which help streamline the deployment and scaling of these applications. These resources allow users to quickly set up and run the examples in local or cloud environments, ensuring a seamless experience."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Prepare the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook uses Docker as an engine, you can deploy this example on any of the available Cloud providers following [THIS]([here](https://opea-project.github.io/latest/getting-started/README.html)) guide . \n",
    "\n",
    "Once your plaftorm is provisioned, follow the below steps to run the example:."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Install Docker Engine (Linux)\n",
    "\n",
    "This is the only requirement to have in your environment since OPEA is based on a microservices architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the command below to install docker:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -fsSL https://get.docker.com -o get-docker.sh\n",
    "!sudo sh get-docker.sh --version 26.1\n",
    "!sudo gpasswd -a devcloud docker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Clone GenAIExamples Repo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned, end-to-end blueprints are provided on OPEA [GenAIExamples repo](link), you can see there other examples available like : AgentsQnA, AudioQnA and MultimodalQnA amoung others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clone the repo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/opea-project/GenAIExamples.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now explore the OPEA ChatQnA RAG deployment. As mentioned, it's a microservices blueprint designed for scalability, resilience, and flexibility. In this task you will explore each microservice, the purpose of exploring each microservice is to help you understand how each component contributes to the overall application. This learning path will guide you through the system, illustrating the role of each service and how they work together.\n",
    "\n",
    "Each service can scale individually based on demand, optimizing resources and performance. Additionally, microservices improve fault isolation—if one service fails, it doesn’t disrupt the entire system. This architecture supports efficient maintenance, rapid updates, and adaptability, making it ideal for responding to changing business needs and user demands.\n",
    "\n",
    "Every OPEA configuration is built on three main parts:\n",
    "\n",
    "![microservices-arch](./Images/microservices-arch.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Megaservice** : Microservice \"orchestrator\". When deploying an end-to-end application with multiple parts involved, there is needed to specify how the flow will be within the microservices. You can learn more from [OPEA documentation](https://github.com/opea-project/GenAIComps?tab=readme-ov-file#megaservice)\n",
    "\n",
    "- **Gateway** : A gateway is the interface for users to access to the `megaservice` It acts as the entry point for incoming requests, routing them to the appropriate Microservices within the megaservice architecture.\n",
    "\n",
    "- **Microservice** : Each individual microservice part of the end-to-end application like : **embeddings**, **retrievers**, **LLM** and **vector databases** among others.\n",
    "\n",
    "You can see below the architecture you will be deploying `ChatQnA`\n",
    "\n",
    "![\"opea-rag\"](./Images/opea_rag.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Set your enviroment variables \n",
    "\n",
    "Each microservice has a set of configurable variables that will specifcy the containter behaviour.For this example, we can specify which Models will be used by the `Embedding`, `LLM` and `reranker`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import json\n",
    "\n",
    "# Set environment variables\n",
    "os.environ[\"host_ip\"] = os.popen(\"hostname -I | awk '{print $1}'\").read().strip()\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"your_huggingface_key\"\n",
    "os.environ[\"EMBEDDING_MODEL_ID\"] = \"BAAI/bge-base-en-v1.5\"\n",
    "os.environ[\"RERANK_MODEL_ID\"] = \"BAAI/bge-reranker-base\"\n",
    "os.environ[\"LLM_MODEL_ID\"] = \"Intel/neural-chat-7b-v3-3\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Start the containers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All containers are already created and you can pick your components for your example based on your needs. This is done through a docker compose file, for this example, we will be using the file available on the repo to use `Milvus` as Vector Data base. You can explore some `docker compose` files provisioned for ChatQnA [HERE](link), or you can build your own to use the multiple available components on [GenAIComps](repo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell output was intentionally hidden.\n",
    "%%capture captured_output\n",
    "\n",
    "!cd GenAIExamples/ChatQnA/docker_compose/intel/cpu/xeon/ && docker compose -f compose_milvus.yaml up -d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Verify Containers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONTAINER ID   IMAGE                                                   COMMAND                  CREATED         STATUS                            PORTS                                                                                  NAMES\n",
      "71b0592c776d   opea/nginx:latest                                       \"/docker-entrypoint.…\"   3 minutes ago   Up 3 minutes                      0.0.0.0:80->80/tcp, :::80->80/tcp                                                      chatqna-xeon-nginx-server\n",
      "ebf6be0c36eb   opea/chatqna-ui:latest                                  \"docker-entrypoint.s…\"   3 minutes ago   Up 3 minutes                      0.0.0.0:5173->5173/tcp, :::5173->5173/tcp                                              chatqna-xeon-ui-server\n",
      "edf2c1bbc933   opea/chatqna:latest                                     \"python chatqna.py\"      3 minutes ago   Up 3 minutes                      0.0.0.0:8888->8888/tcp, :::8888->8888/tcp                                              chatqna-xeon-backend-server\n",
      "1a38bbb020f4   opea/dataprep:latest                                    \"sh -c 'python $( [ …\"   3 minutes ago   Up 3 minutes                      0.0.0.0:6007->5000/tcp, :::6007->5000/tcp                                              dataprep-redis-server\n",
      "1b1a397fd69c   opea/retriever:latest                                   \"python opea_retriev…\"   3 minutes ago   Up 3 minutes                      0.0.0.0:7000->7000/tcp, :::7000->7000/tcp                                              retriever-redis-server\n",
      "e309fc231717   opea/vllm:latest                                        \"python3 -m vllm.ent…\"   3 minutes ago   Up 3 minutes (health: starting)   0.0.0.0:9009->80/tcp, :::9009->80/tcp                                                  vllm-service\n",
      "24f0bd784130   ghcr.io/huggingface/text-embeddings-inference:cpu-1.5   \"text-embeddings-rou…\"   3 minutes ago   Up 3 minutes                      0.0.0.0:6006->80/tcp, :::6006->80/tcp                                                  tei-embedding-server\n",
      "bf2f4c9c83dc   ghcr.io/huggingface/text-embeddings-inference:cpu-1.5   \"text-embeddings-rou…\"   3 minutes ago   Up 3 minutes                      0.0.0.0:8808->80/tcp, :::8808->80/tcp                                                  tei-reranking-server\n",
      "0434f25c969f   redis/redis-stack:7.2.0-v9                              \"/entrypoint.sh\"         3 minutes ago   Up 3 minutes                      0.0.0.0:6379->6379/tcp, :::6379->6379/tcp, 0.0.0.0:8001->8001/tcp, :::8001->8001/tcp   redis-vector-db\n"
     ]
    }
   ],
   "source": [
    "# You should be able to see 8 containers\n",
    "!docker ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-10 15:06:36 __init__.py:190] Automatically detected platform cpu.\n",
      "WARNING 02-10 15:06:37 _logger.py:72] Torch Profiler is enabled in the API server. This should ONLY be used for local development!\n",
      "INFO 02-10 15:06:37 api_server.py:840] vLLM API server version 0.1.dev1+gfde7126\n",
      "INFO 02-10 15:06:37 api_server.py:841] args: Namespace(host='0.0.0.0', port=80, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, lora_modules=None, prompt_adapters=None, chat_template=None, chat_template_content_format='auto', response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_request_id_headers=False, enable_auto_tool_choice=False, enable_reasoning=False, reasoning_parser=None, tool_call_parser=None, tool_parser_plugin='', model='Intel/neural-chat-7b-v3-3', task='auto', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, allowed_local_media_path=None, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', max_model_len=None, guided_decoding_backend='xgrammar', logits_processor_pattern=None, model_impl='auto', distributed_executor_backend=None, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=None, enable_prefix_caching=None, disable_sliding_window=False, use_v2_block_manager=True, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.9, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_seqs=None, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, hf_overrides=None, enforce_eager=False, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, enable_lora=False, enable_lora_bias=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, scheduling_policy='fcfs', override_neuron_config=None, override_pooler_config=None, compilation_config=None, kv_transfer_config=None, worker_cls='auto', generation_config=None, override_generation_config=None, enable_sleep_mode=False, calculate_kv_scales=False, disable_log_requests=False, max_log_len=None, disable_fastapi_docs=False, enable_prompt_tokens_details=False)\n",
      "INFO 02-10 15:06:37 api_server.py:206] Started engine process with PID 41\n",
      "INFO 02-10 15:06:43 __init__.py:190] Automatically detected platform cpu.\n",
      "WARNING 02-10 15:06:44 _logger.py:72] Torch Profiler is enabled in the API server. This should ONLY be used for local development!\n",
      "INFO 02-10 15:06:45 config.py:542] This model supports multiple tasks: {'embed', 'reward', 'classify', 'score', 'generate'}. Defaulting to 'generate'.\n",
      "WARNING 02-10 15:06:45 config.py:678] Async output processing is not supported on the current platform type cpu.\n",
      "WARNING 02-10 15:06:45 _logger.py:72] CUDA graph is not supported on CPU, fallback to the eager mode.\n",
      "WARNING 02-10 15:06:45 _logger.py:72] Environment variable VLLM_CPU_KVCACHE_SPACE (GB) for CPU backend is not set, using 4 by default.\n",
      "WARNING 02-10 15:06:45 _logger.py:72] uni is not supported on CPU, fallback to mp distributed executor backend.\n",
      "INFO 02-10 15:06:52 config.py:542] This model supports multiple tasks: {'embed', 'classify', 'generate', 'score', 'reward'}. Defaulting to 'generate'.\n",
      "WARNING 02-10 15:06:52 config.py:678] Async output processing is not supported on the current platform type cpu.\n",
      "WARNING 02-10 15:06:52 _logger.py:72] CUDA graph is not supported on CPU, fallback to the eager mode.\n",
      "WARNING 02-10 15:06:52 _logger.py:72] Environment variable VLLM_CPU_KVCACHE_SPACE (GB) for CPU backend is not set, using 4 by default.\n",
      "WARNING 02-10 15:06:52 _logger.py:72] uni is not supported on CPU, fallback to mp distributed executor backend.\n",
      "INFO 02-10 15:06:52 importing.py:16] Triton not installed or not compatible; certain GPU-related functions will not be available.\n",
      "INFO 02-10 15:06:52 llm_engine.py:234] Initializing a V0 LLM engine (v0.1.dev1+gfde7126) with config: model='Intel/neural-chat-7b-v3-3', speculative_config=None, tokenizer='Intel/neural-chat-7b-v3-3', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cpu, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=Intel/neural-chat-7b-v3-3, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=True, \n",
      "WARNING 02-10 15:06:52 _logger.py:72] Reducing Torch parallelism from 32 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 02-10 15:06:52 cpu.py:40] Using Torch SDPA backend.\n",
      "INFO 02-10 15:06:52 cpu_worker.py:189] Profiling enabled. Traces will be saved to: /mnt\n",
      "INFO 02-10 15:06:53 weight_utils.py:254] Using model weights format ['*.safetensors']\n",
      "INFO 02-10 15:10:50 weight_utils.py:272] Time took to download weights for Intel/neural-chat-7b-v3-3: 236.588419 seconds\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:09<00:09,  9.76s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:27<00:00, 14.38s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:27<00:00, 13.68s/it]\n",
      "\n",
      "INFO 02-10 15:11:17 executor_base.py:110] # CPU blocks: 2048, # CPU blocks: 0\n",
      "INFO 02-10 15:11:17 executor_base.py:115] Maximum concurrency for 32768 tokens per request: 1.00x\n",
      "INFO 02-10 15:11:20 llm_engine.py:431] init engine (profile, create kv cache, warmup model) took 2.81 seconds\n",
      "INFO 02-10 15:11:20 api_server.py:756] Using supplied chat template:\n",
      "INFO 02-10 15:11:20 api_server.py:756] None\n",
      "INFO 02-10 15:11:20 launcher.py:21] Available routes are:\n",
      "INFO 02-10 15:11:20 launcher.py:29] Route: /openapi.json, Methods: HEAD, GET\n",
      "INFO 02-10 15:11:20 launcher.py:29] Route: /docs, Methods: HEAD, GET\n",
      "INFO 02-10 15:11:20 launcher.py:29] Route: /docs/oauth2-redirect, Methods: HEAD, GET\n",
      "INFO 02-10 15:11:20 launcher.py:29] Route: /redoc, Methods: HEAD, GET\n",
      "INFO 02-10 15:11:20 launcher.py:29] Route: /health, Methods: GET\n",
      "INFO 02-10 15:11:20 launcher.py:29] Route: /ping, Methods: GET, POST\n",
      "INFO 02-10 15:11:20 launcher.py:29] Route: /tokenize, Methods: POST\n",
      "INFO 02-10 15:11:20 launcher.py:29] Route: /detokenize, Methods: POST\n",
      "INFO 02-10 15:11:20 launcher.py:29] Route: /v1/models, Methods: GET\n",
      "INFO 02-10 15:11:20 launcher.py:29] Route: /version, Methods: GET\n",
      "INFO 02-10 15:11:20 launcher.py:29] Route: /v1/chat/completions, Methods: POST\n",
      "INFO 02-10 15:11:20 launcher.py:29] Route: /v1/completions, Methods: POST\n",
      "INFO 02-10 15:11:20 launcher.py:29] Route: /v1/embeddings, Methods: POST\n",
      "INFO 02-10 15:11:20 launcher.py:29] Route: /pooling, Methods: POST\n",
      "INFO 02-10 15:11:20 launcher.py:29] Route: /score, Methods: POST\n",
      "INFO 02-10 15:11:20 launcher.py:29] Route: /v1/score, Methods: POST\n",
      "INFO 02-10 15:11:20 launcher.py:29] Route: /rerank, Methods: POST\n",
      "INFO 02-10 15:11:20 launcher.py:29] Route: /v1/rerank, Methods: POST\n",
      "INFO 02-10 15:11:20 launcher.py:29] Route: /v2/rerank, Methods: POST\n",
      "INFO 02-10 15:11:20 launcher.py:29] Route: /invocations, Methods: POST\n",
      "INFO 02-10 15:11:20 launcher.py:29] Route: /start_profile, Methods: POST\n",
      "INFO 02-10 15:11:20 launcher.py:29] Route: /stop_profile, Methods: POST\n",
      "INFO:     Started server process [1]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://0.0.0.0:80 (Press CTRL+C to quit)\n",
      "INFO:     172.18.0.1:42070 - \"GET /health HTTP/1.1\" 200 OK\n",
      "INFO:     172.18.0.1:43368 - \"GET /health HTTP/1.1\" 200 OK\n",
      "INFO:     172.18.0.1:34230 - \"GET /health HTTP/1.1\" 200 OK\n",
      "INFO:     172.18.0.1:46874 - \"GET /health HTTP/1.1\" 200 OK\n"
     ]
    }
   ],
   "source": [
    "#Check vLLM - Wait Until you see vLLM started complete\n",
    "\n",
    "!docker logs vllm-service 2>&1 | grep complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Test your application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ChatQnA is an end-to-end blueprint, which means that in addition to the components mentioned before, you will also have an UI to interact with your application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Use the UI \n",
    "\n",
    "To test on your browser:\n",
    "\n",
    "1. Copy your Jupyter LAB URL, for example:\n",
    "\n",
    "`https://tiber-opea-workshop-1.eglb.intel.com/2/lab/tree/opea.ipynb`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Get your public token for the URL. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.environ[\"PUBLICURL\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Build you new URL, adding the PUBLICURL value at the end and using the same format\n",
    "\n",
    "`https://tiber-opea-workshop-1.eglb.intel.com/opea/2/95ca4f808974c40bead0d6eebe10714a/`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. You should be able to see the public UI\n",
    "![IMAGE](./Images/opea_ui.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now interact with the application, we can ask things like **\"What is Nike's revenue in 2023?\"**.\n",
    "\n",
    "Let's now explore each microservice and their behaviour, this will help you understand how they interact and how the RAG example is built"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Prompt from the megaservice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_chatqna(query: str, verbose: bool = True) -> str:\n",
    "    # Define the API endpoint and headers\n",
    "    url = \"http://localhost/v1/chatqna\"\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "    payload = {\"messages\": \"What is the revenue of Nike in 2023?\"}\n",
    "\n",
    "    # Make the request and stream the response\n",
    "    content = \"\"\n",
    "    with requests.post(url, json=payload, headers=headers, stream=True) as response:\n",
    "        for line in response.iter_lines():\n",
    "            if line:  # Ignore empty lines\n",
    "                decoded_line = line.decode(\"utf-8\")\n",
    "                if decoded_line == \"[DONE]\":\n",
    "                    break  # Stop when the stream is done\n",
    "                decoded_clean = (\n",
    "                    decoded_line.replace(\"data: b'\", \"\")\n",
    "                    .replace(\"'\", \"\")\n",
    "                    .replace(\"data: [DONE]\", \"\")\n",
    "                )\n",
    "                content += decoded_clean\n",
    "                if verbose:\n",
    "                    print(decoded_clean, end=\"\", flush=True)\n",
    "\n",
    "    return content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should receive an answer back, verifying that all the services in the RAG flow are working, remember that the microservice is the point of entry and interacts with all the internal services, if you recieve an answer will mean that the blueprint is working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query ChatQnA without RAG\n",
    "query = \"What was the revenue of Nike in 2023?\"\n",
    "content = query_chatqna(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "You will see that the model couldn’t answer the question because it lacks the necessary context and operates on ***outdated information***. Without up-to-date or relevant data, it’s unable to provide accurate responses. You'll fix this in few minutes by using RAG to allow the model to pull in current, contextually relevant information, ensuring more precise and relevant answers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Provide external context before prompting "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, you will be feeding the application with context information. From the previous query, the model didn't know **\"What is the revenue of Nike in 2023?\"**, affortunately, if we have a document containing that information we could feed our application and have it correclty answered. The process then,  is to start from a document (Nike's revenue PDF), and do the preprocessing needed to make it ready to be stored in a database. As shown, this process primarily involves 3 microservices: `data preparation`, `embeddings` and `vector store`. Let's explore each microservice\n",
    "<p align=\"center\">\n",
    "  <img src=\"../Images/pre_flow.png\" alt=\"Alt text\" />\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.1 Embedding Microservice (Container:chatqna-tei:80)\n",
    "\n",
    "The document has to be ingested in a way that operations like similarities and storage can be done. Documents aren't stored with words, they are stored as `embeddings`.\n",
    "An **embedding** is a numerical representation of an object—such as a word, phrase, or document in a continuous vector space. In the context of natural language processing (NLP), embeddings represent words, sentences, or other pieces of text as a set of numbers (or a \"vector\") that capture their meaning, relationships, and context. By transforming text into this format, embeddings make it easier for machine learning models to understand and work with text data.\n",
    "\n",
    "For example, the following image shows how word embeddings represent words as points in a vector space based on their relationships. Words with similar meanings, like \"king\" and \"queen\" are closer together, and the embedding model captures these connections through vector arithmetic. \n",
    "\n",
    "During training, if the model sees \"king\" often used with \"man\" and \"queen\" with \"woman,\" it learns that \"king\" and \"queen\" relate similarly to \"man\" and \"woman.\" So, it positions these words in ways that reflect gender relationships in language.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"./Images/king_vs_queen.png\" alt=\"Alt text\" />\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embeddings are a key component for RAG:\n",
    "\n",
    "•\t***Capturing Meaning***: Embeddings represent the semantic relationships between words, allowing RAG models to understand context and nuances in language, enhancing their ability to generate relevant responses.\n",
    "\n",
    "•\t***Dimensionality Reduction***: By converting complex information into fixed-size vectors, embeddings streamline data processing, making RAG systems more efficient and faster.\n",
    "\n",
    "•\t***Improving Model Performance***: Embeddings enable RAG models to generalize better by leveraging semantic similarities, facilitating more accurate information retrieval, and improving the quality of generated content.\n",
    "\n",
    "OPEA provides multiple options to run your embeddings microservice, as detailed in the [OPEA embedding documentation](https://github.com/opea-project/GenAIComps/tree/main/comps/embeddings): \n",
    "\n",
    "In this case, ChatQnA uses [Hugging Face TEI](https://huggingface.co/docs/text-embeddings-inference/en/index) microservice running the embedding model `BAAI/bge-large-en-v1.5` locally. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the microservice by getting the embedding for the phrase \"What was Deep Learning?\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl http://localhost:6006/embed -X POST -d \\\n",
    "    '{\"inputs\":\"What is Deep Learning?\"}' -H \\\n",
    "    'Content-Type: application/json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2. Store the public Nike report for 2023 in the Vector Database (chatqna-data-prep:6007)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned, this example uses Milvus as the vector database. You can find all of the supported alternatives in the [OPEA vector store repository](https://github.com/opea-project/GenAIComps/tree/main/comps/vectorstores)\n",
    "\n",
    "A Vector Database (VDB) is a specialized database designed to store and manage high-dimensional vectors—numeric representations of data points like words, sentences, or images. In AI and machine learning, these vectors are typically embeddings, which capture the meaning and relationships of data in a format that algorithms can process efficiently, as we have shown before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Microservice involved on storing the emebddings to the vdb is the `Dataprep` Microservice. It is responsible for preparing data in a digestible format for the application, converting it to embeddings (using the embedding microservice) and loading it to the database. This service preprocesses/transforms the data, making sure it is clean, organized, and suitable for further processing.  \n",
    "\n",
    "Specifically, this microservice receives data (such as documents), processes it by breaking it into chunks, sends it to the embedding microservice, and stores these vectors in the vector database. The microservice's functionality may depend on the specific vector database being used, as each database has its own requirements for data formatting\n",
    "\n",
    "To test it and help the model answer the initial question **What was Nike revenue in 2023?**, you will need to upload a context file (revenue report) to be processed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Execute the following command to download a sample [Nike revenue report](https://github.com/opea-project/GenAIComps/blob/main/comps/retrievers/redis/data/nke-10k-2023.pdf) to the nginx pod (if you are no longer logged in to the NGinx pod, be sure to use the above command to log in again):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ingest your data for your RAG. In this case NIke's revenue document\n",
    "\n",
    "!wget https://raw.githubusercontent.com/opea-project/GenAIComps/v1.1/comps/retrievers/redis/data/nke-10k-2023.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Run the ingestion function on the `dataprep` microservice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -X POST \"http://localhost:11101/v1/dataprep/ingest\" \\\n",
    "     -H \"Content-Type: multipart/form-data\" \\\n",
    "     -F \"files=@./nke-10k-2023.pdf\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Run the get function to retrieve all documents ingested on the vdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can check if the file was successfully updated\n",
    "!curl -X POST \"http://localhost:11101/v1/dataprep/get\" \\\n",
    "     -H \"Content-Type: application/json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Test if your RAG application provide answer based on context (Nike revenue in 2023)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Run your query (Directly to the LLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "# You can directly check the LLM,\n",
    "!curl http://${host_ip}:9009/v1/chat/completions \\\n",
    "  -X POST \\\n",
    "  -d '{\"model\": \"Intel/neural-chat-7b-v3-3\", \"messages\": [{\"role\": \"user\", \"content\": \"What is Deep Learning?\"}], \"max_tokens\":17}' \\\n",
    "  -H 'Content-Type: application/json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can verify that the model doesn't answered correctly, but WHY? \n",
    "\n",
    "The answer is simple, the LLM isn't re trained with the context, the information you added in the previous step (Nike revenue) is on the vector database. This information is given as a context (new prompt) to the LLM when is prompted from the entry point `megaservice` which will start the process of recieving the answer, finding the most similar documents (chunks) on the vector database using the `retriever` microservice and finally creating a new prompt that is ingested to the LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Query ChatQnA WITH RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is the revenue of Nike in 2023?\"\n",
    "content = query_chatqna(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Be involved with OPEA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OPEA is an open source project that welcomes contributions in many forms. Whether you're interested in fixing bugs, adding new GenAI components, improving documentation, or sharing your unique use cases, there are numerous ways to get involved and make a meaningful impact. \n",
    "\n",
    "We’re excited to have you on board and look forward to the contributions you'll bring to the OPEA platform. \n",
    "\n",
    "For detailed instructions on how to contribute, please check out our [Contributing Documentation](https://opea-project.github.io/latest/community/CONTRIBUTING.html).\n",
    "\n",
    "[Follow the project](https://github.com/opea-project) and stay tuned for new events!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
